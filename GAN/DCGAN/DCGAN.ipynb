{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracted Information From the Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pytorch.org/tutorials/_images/dcgan_generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On paper, kernal size = 5 is used, but dimensions does not align for the discriminator later (require padding = 3/2 for stride = 2 to be used as well)\n",
    "\n",
    "=> Thus, kernal size used will be 4 instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. DCGAN Generator (from Figure)\n",
    "\n",
    "- **Input**: 100-dimensional uniform distribution (Z)\n",
    "- **Layers**: A series of four fractionally-strided convolutions  \n",
    "  *(Note: These are sometimes mistakenly referred to as deconvolutions in recent papers)*  \n",
    "  *[Configuration as per figure: stride = 2, kernal size = 4, use formulas to calcuate the padding required]* \n",
    "- **Output**: Converts the high-level representation into a 64 × 64 pixel image.\n",
    "\n",
    "\n",
    "### 2. Architecture Guidelines for Stable Deep Convolutional GANs\n",
    "\n",
    "- **Replace pooling layers**:\n",
    "  - Use **strided convolutions** in the discriminator.\n",
    "  - Use **fractional-strided convolutions** in the generator.  \n",
    "  *(The discriminator mirrors the generator.)*\n",
    "  \n",
    "- **Batch normalization**:\n",
    "  - Apply **batchnorm** in both the generator and discriminator.\n",
    "  - DO NOT APPLY batchnorm to the generator output layer and the discriminator input layer.\n",
    "  \n",
    "- **Deeper architectures**:\n",
    "  - Remove **fully connected hidden layers**.\n",
    "\n",
    "- **Activation functions**:\n",
    "  - Use **ReLU** activation in the generator for all layers, except the output, which uses **Tanh**.\n",
    "  - Use **LeakyReLU** activation in the discriminator for all layers.  \n",
    "    *(Leak slope set to 0.2 in all models.)*\n",
    "\n",
    "\n",
    "### 3. Adversarial Training\n",
    "\n",
    "- **Image preprocessing**:  \n",
    "  - Resize image dimensions to 64 by 64\n",
    "  - No preprocessing other than scaling to the range of the Tanh activation function \\([-1, 1]\\).  \n",
    "    *(To match the Generator's Tanh output.)*\n",
    "  \n",
    "- **Batch size**: 128\n",
    "\n",
    "- **Weight initialization**:  \n",
    "  - Weights are initialized from a **zero-centered Normal Distribution** with a standard deviation of 0.02.\n",
    "\n",
    "- **Optimizer**:  \n",
    "  - **Adam Optimizer** with learning rate lr = 0.0002 and momentum term B_1 = 0.5\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create DCGAN's Generator and Discriminator Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fractional Strided Convolutional Layers for the Generator\n",
    "# - Configuration as per figure: stride = 2, kernal size = 4\n",
    "# - Use formula to calcuate the padding required:\n",
    "#       H[out] = (H[in] - 1) * stride - 2 * padding + kernal_size + output_padding\n",
    "#   => output_padding - 2*padding = -2 for all cases \n",
    "#   => Let output_padding = 0, padding = 1\n",
    "#   => This config will x2 to the image dim (img_height and img width) for each convultional layer\n",
    "\n",
    "# Generator:\n",
    "# 1. Input: 100-dimensional uniform distribution (Z)\n",
    "# 2. Projection layer: to 1024*4*4 (to be reshaped before sending to f-s convolutional layers)\n",
    "# 3. A series of four fractionally-strided convolutions  (stride = 2, kernal size = 4) \n",
    "#   3.1 f-s conv: Output Chanels = 512, img_dim from 4*4 to 8*8\n",
    "#   3.2 f-s conv: Output Chanels = 256, img_dim from 8*8 to 16*16\n",
    "#   3.3 f-s conv: Output Chanels = 128, img_dim from 4*4 to 32*32\n",
    "#   3.4. [Output]: f-s conv: Output Chanels = 3, img_dim from 32*32 to 64*64\n",
    "# *Batch Norm to be applied (except last layer of generator)\n",
    "# *ReLU all layers (except Output layer)\n",
    "# *TanH for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, noise_channels=100, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "\n",
    "            # 1st fractional strided convolution layer (upsample from 1*1 -> 4*4)\n",
    "            # Projection layer, to convert the z of 100 inputs to 1024 * 4 * 4 (noise_channels = z_dim)\n",
    "            # Each input (z) will be actually reshaped to 100 * 1 * 1 (100 channels)\n",
    "            # (to ensure from 1x1 -> 4x4, with stride = 2 and kernal = 4, we need padding = 0 now (for a x4 increase))\n",
    "            self._block(in_channels=noise_channels, out_channels=1024, kernel_size=4, stride=2, padding=0),\n",
    "\n",
    "            # 2nd fractional strided convolution layer (upsample from 4*4 -> 8*8)\n",
    "            self._block(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # 3rd fractional strided convolution layer (upsample from 8*8 -> 16*16)\n",
    "            self._block(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            \n",
    "            # 4th fractional strided convolution layer (upsample from 16*16 -> 32*32)\n",
    "            self._block(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # Output fractional strided convolution layer (upsample from 32*32 -> 64*64)\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm=True):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        ) if batch_norm else nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.conv_layers(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Generator                                [32, 3, 64, 64]           --\n",
       "├─Sequential: 1-1                        [32, 3, 64, 64]           --\n",
       "│    └─Sequential: 2-1                   [32, 1024, 4, 4]          --\n",
       "│    │    └─ConvTranspose2d: 3-1         [32, 1024, 4, 4]          1,639,424\n",
       "│    │    └─BatchNorm2d: 3-2             [32, 1024, 4, 4]          2,048\n",
       "│    │    └─ReLU: 3-3                    [32, 1024, 4, 4]          --\n",
       "│    └─Sequential: 2-2                   [32, 512, 8, 8]           --\n",
       "│    │    └─ConvTranspose2d: 3-4         [32, 512, 8, 8]           8,389,120\n",
       "│    │    └─BatchNorm2d: 3-5             [32, 512, 8, 8]           1,024\n",
       "│    │    └─ReLU: 3-6                    [32, 512, 8, 8]           --\n",
       "│    └─Sequential: 2-3                   [32, 256, 16, 16]         --\n",
       "│    │    └─ConvTranspose2d: 3-7         [32, 256, 16, 16]         2,097,408\n",
       "│    │    └─BatchNorm2d: 3-8             [32, 256, 16, 16]         512\n",
       "│    │    └─ReLU: 3-9                    [32, 256, 16, 16]         --\n",
       "│    └─Sequential: 2-4                   [32, 128, 32, 32]         --\n",
       "│    │    └─ConvTranspose2d: 3-10        [32, 128, 32, 32]         524,416\n",
       "│    │    └─BatchNorm2d: 3-11            [32, 128, 32, 32]         256\n",
       "│    │    └─ReLU: 3-12                   [32, 128, 32, 32]         --\n",
       "│    └─ConvTranspose2d: 2-5              [32, 3, 64, 64]           6,147\n",
       "│    └─Tanh: 2-6                         [32, 3, 64, 64]           --\n",
       "==========================================================================================\n",
       "Total params: 12,660,355\n",
       "Trainable params: 12,660,355\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 53.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 128.97\n",
       "Params size (MB): 50.64\n",
       "Estimated Total Size (MB): 179.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Generator\n",
    "batch_size = 32\n",
    "z_dim = 100\n",
    "z_dummy = torch.randn(size=(batch_size, z_dim))\n",
    "z_dummy = z_dummy.view(-1, 100, 1, 1)\n",
    "print(z_dummy.shape)\n",
    "\n",
    "generator = Generator(img_channels=3, noise_channels=z_dim)\n",
    "torchinfo.summary(model=generator, input_size=[z_dummy.shape]) # ensure output shape = [batch size * 3 * 64 * 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strided Convolutional Layer for the Discriminator\n",
    "# - Configuration as per figure: stride = 2, kernal size = 4\n",
    "# - Use formula to calcuate the padding required:\n",
    "#       H[out] = [ (H[in] - kernal_size + 2 * padding) / stride ] + 1\n",
    "\n",
    "#   => padding = 1 for all cases \n",
    "#   => Let padding = 1\n",
    "#   => This config will x2 to the image dim (img_height and img width) for each convultional layer\n",
    "\n",
    "# Discriminator: (just the mirror opposite of the configuration, with strided convolutional layers instead of fractional strided convolutional layers)\n",
    "# 1. Input: (3 by 64 by 64) images\n",
    "# 2. A series of four strided convolutions  (stride = 2, kernal size = 4) \n",
    "#   2.1 f-s conv: Output Chanels = 128, img_dim from 64*64 to 32*32\n",
    "#   2.2 f-s conv: Output Chanels = 256, img_dim from 32*32 to 16*16 \n",
    "#   2.3 f-s conv: Output Chanels = 512, img_dim from 16*16 to 8*8\n",
    "#   2.4. [Output]: f-s conv: Output Chanels = 1024, img_dim from 8*8 to 4*4\n",
    "# *Batch Norm to be applied (except first layer for the discriminator)\n",
    "# *LeakyReLU all layers, slope set to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            \n",
    "            # 1st fractional strided convolution layer (downsample from 64*64 -> 32*32)\n",
    "            self._block(in_channels=img_channels, out_channels=128, kernel_size=4, stride=2, padding=1, batch_norm=False),\n",
    "\n",
    "            # 2nd fractional strided convolution layer (downsample from 32*32 -> 16*16)\n",
    "            self._block(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            \n",
    "            # 3rd fractional strided convolution layer (downsample from 16*16 -> 8*8)\n",
    "            self._block(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "            # Output fractional strided convolution layer (downsample from 8*8 -> 4*4)\n",
    "            self._block(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "            \n",
    "            # Classifier\n",
    "            # No fully connected layer for DCGAN, use another way (instead of nn.Flatten(), nn.Linear(in_features=1024*4*4, out_features=1))\n",
    "            # Use another convolutional layer (to ensure from 4x4 to 1x1, with stride = 2 and kernal = 4, we need padding = 0 now (for a x4 reduction))\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid() # ensure prediction is within [0, 1]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm=True):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        ) if batch_norm else nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Discriminator                            [32, 1, 1, 1]             --\n",
       "├─Sequential: 1-1                        [32, 1, 1, 1]             --\n",
       "│    └─Sequential: 2-1                   [32, 128, 32, 32]         --\n",
       "│    │    └─Conv2d: 3-1                  [32, 128, 32, 32]         6,272\n",
       "│    │    └─LeakyReLU: 3-2               [32, 128, 32, 32]         --\n",
       "│    └─Sequential: 2-2                   [32, 256, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-3                  [32, 256, 16, 16]         524,544\n",
       "│    │    └─BatchNorm2d: 3-4             [32, 256, 16, 16]         512\n",
       "│    │    └─LeakyReLU: 3-5               [32, 256, 16, 16]         --\n",
       "│    └─Sequential: 2-3                   [32, 512, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-6                  [32, 512, 8, 8]           2,097,664\n",
       "│    │    └─BatchNorm2d: 3-7             [32, 512, 8, 8]           1,024\n",
       "│    │    └─LeakyReLU: 3-8               [32, 512, 8, 8]           --\n",
       "│    └─Sequential: 2-4                   [32, 1024, 4, 4]          --\n",
       "│    │    └─Conv2d: 3-9                  [32, 1024, 4, 4]          8,389,632\n",
       "│    │    └─BatchNorm2d: 3-10            [32, 1024, 4, 4]          2,048\n",
       "│    │    └─LeakyReLU: 3-11              [32, 1024, 4, 4]          --\n",
       "│    └─Conv2d: 2-5                       [32, 1, 1, 1]             16,385\n",
       "│    └─Sigmoid: 2-6                      [32, 1, 1, 1]             --\n",
       "==========================================================================================\n",
       "Total params: 11,038,081\n",
       "Trainable params: 11,038,081\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 13.09\n",
       "==========================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 92.27\n",
       "Params size (MB): 44.15\n",
       "Estimated Total Size (MB): 138.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Discriminator\n",
    "batch_size = 32\n",
    "img_dim = 64\n",
    "img_channels=3\n",
    "x_dummy = torch.randn(size=(batch_size, 3, img_dim, img_dim))\n",
    "print(x_dummy.shape)\n",
    "\n",
    "discriminator = Discriminator(img_channels=3)\n",
    "torchinfo.summary(model=discriminator, input_size=[x_dummy.shape]) # ensure output shape = [batch size * 1 * 1 * 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights are initialized from Normal Distribution with mean = 0; standard deviation = 0.02.\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(img_channels=3)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "\n",
    "    z_dim = 100\n",
    "    z = torch.randn((N, z_dim, 1, 1))\n",
    "    gen = Generator(noise_channels=z_dim, img_channels=3)\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "\n",
    "    print(\"Success, tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, tests passed!\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Following the DCGAN paper\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 64\n",
    "IMG_CHANNELS = 3 # can be changed wrt to images (althought DCGAN paper, input channels of images is to be 3)\n",
    "Z_DIM = 100\n",
    "LEARNING_RATE = 2e-4\n",
    "B1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformations\n",
    "# Ensure:\n",
    "# 1. Image is resized to 64*64\n",
    "# 2. Ensure that Input Images are normalised such that they are within [-1, 1] (to follow generator's tanh output of [-1, 1])\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(IMG_CHANNELS)],\n",
    "            [0.5 for _ in range(IMG_CHANNELS)]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create the dataset\n",
    "data_dir = Path().cwd().parent.parent / \"data\"\n",
    "image_folder_name = \"celeb_A\"\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir/image_folder_name, transform=transforms)\n",
    "\n",
    "# Create the dataloader\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_sample = x[0]\n",
    "# x_sample = x_sample.permute(1, 2, 0).numpy()\n",
    "# plt.imshow(x_sample)\n",
    "# x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Models\n",
    "\n",
    "generator = Generator(noise_channels=Z_DIM, img_channels=IMG_CHANNELS).to(device)\n",
    "discriminator = Discriminator(img_channels=IMG_CHANNELS).to(device)\n",
    "\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_D = optim.Adam(params=discriminator.parameters(), lr=LEARNING_RATE, betas=(B1, 0.999)) # b2 kept as default\n",
    "optimizer_G = optim.Adam(params=generator.parameters(), lr=LEARNING_RATE, betas=(B1, 0.999))  # b2 kept as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"runs/MNIST/real\")\n",
    "writer_fake = SummaryWriter(f\"runs/MNIST/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/469                   Loss D: 1.4009, loss G: 0.9226\n",
      "Epoch [0/10] Batch 100/469                   Loss D: 0.0090, loss G: 5.4320\n",
      "Epoch [0/10] Batch 200/469                   Loss D: 0.0425, loss G: 4.1582\n",
      "Epoch [0/10] Batch 300/469                   Loss D: 0.9446, loss G: 2.1268\n",
      "Epoch [0/10] Batch 400/469                   Loss D: 1.0685, loss G: 1.3641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:49<34:24, 229.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Batch 0/469                   Loss D: 0.9675, loss G: 2.0899\n",
      "Epoch [1/10] Batch 100/469                   Loss D: 0.9081, loss G: 1.0731\n",
      "Epoch [1/10] Batch 200/469                   Loss D: 0.8497, loss G: 1.0624\n",
      "Epoch [1/10] Batch 300/469                   Loss D: 0.9271, loss G: 0.9187\n",
      "Epoch [1/10] Batch 400/469                   Loss D: 0.6022, loss G: 3.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [07:38<30:32, 229.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Batch 0/469                   Loss D: 0.7099, loss G: 3.3171\n",
      "Epoch [2/10] Batch 100/469                   Loss D: 1.1726, loss G: 3.9874\n",
      "Epoch [2/10] Batch 200/469                   Loss D: 0.1685, loss G: 3.1225\n",
      "Epoch [2/10] Batch 300/469                   Loss D: 0.3734, loss G: 3.8617\n",
      "Epoch [2/10] Batch 400/469                   Loss D: 0.4614, loss G: 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [11:26<26:41, 228.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Batch 0/469                   Loss D: 0.4336, loss G: 2.6499\n",
      "Epoch [3/10] Batch 100/469                   Loss D: 0.5677, loss G: 3.1196\n",
      "Epoch [3/10] Batch 200/469                   Loss D: 0.5899, loss G: 5.7335\n",
      "Epoch [3/10] Batch 300/469                   Loss D: 1.3081, loss G: 1.6189\n",
      "Epoch [3/10] Batch 400/469                   Loss D: 0.1410, loss G: 3.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [15:15<22:53, 228.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Batch 0/469                   Loss D: 0.3334, loss G: 3.2590\n",
      "Epoch [4/10] Batch 100/469                   Loss D: 0.8403, loss G: 2.9569\n",
      "Epoch [4/10] Batch 200/469                   Loss D: 0.2083, loss G: 3.1357\n",
      "Epoch [4/10] Batch 300/469                   Loss D: 0.7014, loss G: 1.7803\n",
      "Epoch [4/10] Batch 400/469                   Loss D: 0.2447, loss G: 2.9042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [19:03<19:02, 228.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Batch 0/469                   Loss D: 1.1111, loss G: 1.1240\n",
      "Epoch [5/10] Batch 100/469                   Loss D: 0.1598, loss G: 4.0268\n",
      "Epoch [5/10] Batch 200/469                   Loss D: 2.8133, loss G: 10.2775\n",
      "Epoch [5/10] Batch 300/469                   Loss D: 0.1051, loss G: 3.9182\n",
      "Epoch [5/10] Batch 400/469                   Loss D: 0.1281, loss G: 3.3564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [22:52<15:14, 228.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Batch 0/469                   Loss D: 0.1476, loss G: 3.1856\n",
      "Epoch [6/10] Batch 100/469                   Loss D: 0.1039, loss G: 4.3778\n",
      "Epoch [6/10] Batch 200/469                   Loss D: 0.1845, loss G: 2.8277\n",
      "Epoch [6/10] Batch 300/469                   Loss D: 0.2607, loss G: 3.8172\n",
      "Epoch [6/10] Batch 400/469                   Loss D: 0.1992, loss G: 3.4054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [26:41<11:26, 228.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Batch 0/469                   Loss D: 0.0659, loss G: 4.5583\n",
      "Epoch [7/10] Batch 100/469                   Loss D: 0.5290, loss G: 4.0268\n",
      "Epoch [7/10] Batch 200/469                   Loss D: 0.8195, loss G: 1.6489\n",
      "Epoch [7/10] Batch 300/469                   Loss D: 0.0881, loss G: 3.1335\n",
      "Epoch [7/10] Batch 400/469                   Loss D: 0.2595, loss G: 4.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [30:29<07:37, 228.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Batch 0/469                   Loss D: 1.3147, loss G: 1.7915\n",
      "Epoch [8/10] Batch 100/469                   Loss D: 0.1230, loss G: 3.2608\n",
      "Epoch [8/10] Batch 200/469                   Loss D: 0.0569, loss G: 4.2130\n",
      "Epoch [8/10] Batch 300/469                   Loss D: 0.3214, loss G: 3.2605\n",
      "Epoch [8/10] Batch 400/469                   Loss D: 0.1070, loss G: 3.8579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [34:19<03:48, 228.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Batch 0/469                   Loss D: 0.0652, loss G: 3.5935\n",
      "Epoch [9/10] Batch 100/469                   Loss D: 0.2918, loss G: 2.6131\n",
      "Epoch [9/10] Batch 200/469                   Loss D: 0.5380, loss G: 1.7338\n",
      "Epoch [9/10] Batch 300/469                   Loss D: 2.5044, loss G: 0.6034\n",
      "Epoch [9/10] Batch 400/469                   Loss D: 0.1918, loss G: 3.3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [38:08<00:00, 228.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(dataloader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        noise = torch.randn(size = (x.shape[0], Z_DIM, 1, 1)).to(device)\n",
    "\n",
    "        g_z = generator(noise) # G(z)\n",
    "        d_x = discriminator(x).reshape(-1) # D(x), reshape from 1*1*1 to 1\n",
    "        d_g_z = discriminator(g_z).reshape(-1) # D(G(z)), reshape from 1*1*1 to 1\n",
    "\n",
    "        ### Train the Discriminator: Min -(log(D(x)) + log(1-D(G(Z)))) <---> Max log(D(x)) + log(1-D(G(Z)))\n",
    "\n",
    "        loss_real_D = criterion(d_x, torch.ones_like(d_x)) # -log(D(X))\n",
    "        loss_fake_D = criterion(d_g_z, torch.zeros_like(d_g_z)) # -log(1-D(G(z)))\n",
    "        loss_D = loss_fake_D + loss_real_D #-(log(D(x)) + log(1-D(G(Z))))\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        loss_D.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_D.step()\n",
    "\n",
    "        ### Train the Generator: Min -log(D(G(z)) <---> Max log(D(G(z))) <---> Min log(1-D(G(z)))\n",
    "        d_g_z_next = discriminator(g_z).reshape(-1) # after training the disc, new D(G(z)), reshape from 1*1*1 to 1\n",
    "        loss_G = criterion(d_g_z_next, torch.ones_like(d_g_z_next)) # -log(D(G(z)))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "         # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_D:.4f}, loss G: {loss_G:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(x[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, noise_channels=100, img_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Projection layer, to convert the z of 100 inputs to 1024 * 4 * 4 (noise_channels = z_dim)\n",
    "        # Each input (z) will be actually reshaped to 100 * 1 * 1 (100 channels)\n",
    "        # (to ensure from 1x1 to 4x4, with stride = 2 and kernal = 4, we need padding = 0 now (for a x4 increase))\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=noise_channels, out_channels=1024, kernel_size=4, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            \n",
    "            # 1st fractional strided convolution layer (upsample from 4*4 -> 8*8)\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2nd fractional strided convolution layer (upsample from 8*8 -> 16*16)\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 3rd fractional strided convolution layer (upsample from 16*16 -> 32*32)\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output fractional strided convolution layer (upsample from 32*32 -> 64*64)\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z_projected = self.projection(z) # project each z from z_dim*1*1 into 1024*4*4\n",
    "        return self.conv_layers(z_projected)\n",
    "    \n",
    "\n",
    "#### DEGRADED (ENSURE NO FULLY CONNECTED LAYER (EVEN FOR PROJECTION))\n",
    "# class Generator(nn.Module):\n",
    "\n",
    "#     def __init__(self, z_dim=100):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.projection = nn.Sequential(\n",
    "#             nn.Linear(in_features=z_dim, out_features=1024*4*4),\n",
    "#             nn.BatchNorm1d(num_features=1024*4*4),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.conv_layers = nn.Sequential(\n",
    "            \n",
    "#             # 1st fractional strided convolution layer (upsample from 4*4 -> 8*8)\n",
    "#             nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             # 2nd fractional strided convolution layer (upsample from 8*8 -> 16*16)\n",
    "#             nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "#             # 3rd fractional strided convolution layer (upsample from 16*16 -> 32*32)\n",
    "#             nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             # Output fractional strided convolution layer (upsample from 32*32 -> 64*64)\n",
    "#             nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, z):\n",
    "#         z_projected = self.projection(z) # project each z into a linear vector 1 by 1024*4*4 (output shape = batch_size by 1024*4*4)\n",
    "#         z_projected_reshaped = z_projected.view(-1, 1024, 4, 4) # to reshape the projection after feeding into the convolutional layers (retain batch size)\n",
    "#         return self.conv_layers(z_projected_reshaped)\n",
    "    \n",
    "\n",
    "# batch_size = 32\n",
    "# z_dim = 100\n",
    "# fixed_z = torch.randn(batch_size, z_dim)\n",
    "\n",
    "# generator = Generator(z_dim=z_dim)\n",
    "\n",
    "# output = generator(fixed_z)\n",
    "\n",
    "# print(output.shape) # ensure [batch size by 3 by 64 by 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            \n",
    "            # 1st fractional strided convolution layer (downsample from 64*64 -> 32*32)\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            # 2nd fractional strided convolution layer (upsample from 32*32 -> 16*16)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            \n",
    "            # 3rd fractional strided convolution layer (upsample from 16*16 -> 8*8)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            # Output fractional strided convolution layer (upsample from 8*8 -> 4*4)\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "        # No fully connected layer for DCGAN, use another way (instead of nn.Flatten(), nn.Linear(in_features=1024*4*4, out_features=1))\n",
    "        # Use another convolutional layer (to ensure from 4x4 to 1x1, with stride = 2 and kernal = 4, we need padding = 0 now (for a x4 reduction))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid() # ensure prediction is within [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.conv_layers(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
